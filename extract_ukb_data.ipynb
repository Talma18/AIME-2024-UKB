{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e887c49-5937-44b4-954d-29422fe791f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "MIN_DRUG_OCCURENCE_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3f704-7400-4455-a2ad-df36b9189633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hand curated field names\n",
    "\n",
    "fields = []\n",
    "fields_by_file = {}  # for helper files\n",
    "\n",
    "for filename in sorted(os.listdir('data/raw/ukb_export/')):\n",
    "    if not filename.startswith('field') or not filename.endswith('txt'):\n",
    "        continue\n",
    "\n",
    "    if filename == 'field.txt':  # avoid field.txt if it was already created\n",
    "        continue \n",
    "    \n",
    "    with open(f'data/raw/ukb_export/{filename}') as f:\n",
    "        field_ids = [int(line.strip()) for line in f.readlines()]\n",
    "        fields += field_ids\n",
    "\n",
    "        fields_by_file[filename] = field_ids\n",
    "\n",
    "with open('data/raw/ukb_export/field.txt', 'wt') as f:\n",
    "    for field_id in fields:\n",
    "        f.write(f'{field_id}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0bfa1-8dc9-4927-9f15-7de52f441666",
   "metadata": {},
   "source": [
    "run data/raw/ukb_export/extract.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15dcf38-c93f-456f-895f-b1ec2b62addd",
   "metadata": {},
   "source": [
    "# Set column dtypes and data encoding for exported columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384715b9-f33b-45bb-b755-796866a93dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoding_df = pd.read_csv('data/raw/helper_files/Codings.tsv', sep='\\t')\n",
    "field_info_df = pd.read_csv('data/raw/helper_files/Data_Dictionary_Showcase.tsv', sep='\\t').set_index('FieldID')\n",
    "\n",
    "exported_df = pd.read_csv('data/raw/ukb_export/data.csv').set_index('eid')\n",
    "\n",
    "# rename 189 to 22189, legacy downloads...\n",
    "exported_df.rename(columns={col: '22'+col for col in exported_df if col.split('-')[0] == '189'}, inplace=True)\n",
    "\n",
    "# drop people with no birth information\n",
    "exported_df = exported_df[~exported_df['52-0.0'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c775fb-94da-403d-9de0-1194d6ba9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dtypes = {}\n",
    "special_values = {}  # for helper files\n",
    "\n",
    "for col in tqdm(exported_df.columns):\n",
    "    field_id = int(col.split('-')[0])\n",
    "    try:\n",
    "        field_info = field_info_df.loc[field_id]\n",
    "    except KeyError:\n",
    "        print('ERROR', field_id)\n",
    "        continue\n",
    "\n",
    "    has_coding = not pd.isna(field_info['Coding'])\n",
    "\n",
    "    if has_coding:\n",
    "        coding = int(field_info['Coding'])\n",
    "\n",
    "    if 'Categorical' in field_info['ValueType']:  # Switch 'Categorical single' columns to pandas categorical dtypes\n",
    "        if coding not in categorical_dtypes:\n",
    "            mapping = {int(row['Value']): row['Meaning'] for _, row in data_encoding_df[data_encoding_df['Coding'] == coding].iterrows()}\n",
    "            categorical_dtypes[coding] = (pd.CategoricalDtype(mapping.values()), mapping)\n",
    "        dtype, mapping = categorical_dtypes[coding]\n",
    "        exported_df[col] = exported_df[col].map(mapping).astype(dtype)\n",
    "\n",
    "    # save 'Integer' and 'Continuous' with special data encodings\n",
    "    if field_info['ValueType'] == 'Integer': \n",
    "        if has_coding:\n",
    "            special_values[field_id] = {int(row['Value']): row['Meaning'] for _, row in data_encoding_df[data_encoding_df['Coding'] == coding].iterrows()}\n",
    "        exported_df[col] = exported_df[col].astype('Int64') # Switch 'Integer' columns to pandas Int64\n",
    "            \n",
    "    elif field_info['ValueType'] == 'Continuous':\n",
    "        if has_coding:\n",
    "            special_values[field_id] = {float(row['Value']): row['Meaning'] for _, row in data_encoding_df[data_encoding_df['Coding'] == coding].iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad3904-5338-4d9b-9a41-c36e5c49ff81",
   "metadata": {},
   "source": [
    "## Calculate estimated date of events missing exact information\n",
    "\n",
    "For various reasons the certain date of some events is not accessible, these field are estimated from available information. The official UKB field ids are used when imputing the missing dates, and the extra information is then dropped\n",
    "\n",
    "Date of birth\n",
    "https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=33\n",
    "\n",
    "Reception Date\n",
    "https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435418f5-4864-47bb-b2b0-8c42e5e69f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "birth_dates = pd.to_datetime(exported_df[['34-0.0', '52-0.0']].progress_apply(lambda row: f\"{row['34-0.0']} {row['52-0.0']}\", axis=1), format='%Y %B')\n",
    "exported_df['33-0.0'] = birth_dates.dt.strftime('%Y-%m')\n",
    "exported_df = exported_df.drop(['34-0.0', '52-0.0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d8c78-0620-493d-a14e-c4b3ae30568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different UKB instances are from different assesments\n",
    "\n",
    "# 0 Initial assessment visit (2006-2010) at which participants were recruited and consent given\n",
    "# 1\trep1\tFirst repeat assessment visit (2012-13)\n",
    "# 2\timg\tImaging visit (2014+)\n",
    "# 3\tirep1\tFirst repeat imaging visit (2019+)\n",
    "\n",
    "assesment_dates = [\n",
    "    (pd.to_datetime('2006-01-01'), pd.to_datetime('2010-12-31')),\n",
    "    (pd.to_datetime('2012-01-01'), pd.to_datetime('2013-12-31')),\n",
    "    (pd.to_datetime('2014-01-01'), pd.to_datetime('NaT')),\n",
    "    (pd.to_datetime('2019-01-01'), pd.to_datetime('NaT')),\n",
    "]\n",
    "\n",
    "assesment_cols = []\n",
    "\n",
    "for c in exported_df:\n",
    "    field_id, instance, array = map(int, c.replace('.', '-').split('-'))\n",
    "    \n",
    "    if field_id != 21003:\n",
    "        continue\n",
    "\n",
    "    assesment_cols.append(c)\n",
    "    \n",
    "    start_date, end_date = assesment_dates[instance]\n",
    "\n",
    "    date_offset = exported_df[c].progress_apply(\n",
    "        lambda x: pd.DateOffset(years=x) if pd.notna(x) else pd.NaT\n",
    "    )\n",
    "    \n",
    "    min_dates = birth_dates + date_offset\n",
    "    max_dates = min_dates + pd.DateOffset(years=1, months=1)\n",
    "    \n",
    "    min_dates[min_dates < start_date] = start_date\n",
    "    max_dates[max_dates > end_date] = end_date\n",
    "\n",
    "    average_dates = min_dates + (max_dates - min_dates) / 2\n",
    "\n",
    "    exported_df[f'{53}-{instance}.{array}'] = average_dates.dt.strftime('%Y-%m-%d')\n",
    "\n",
    "exported_df = exported_df.drop(assesment_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86331a-c03d-4f52-98f5-f1a624506e63",
   "metadata": {},
   "source": [
    "# Add events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009802b-06cb-4909-a783-b07ae621da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_event_df(event_df):\n",
    "    # https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=819\n",
    "    date_indicating_missing_information = [\n",
    "        '1900-01-01',\n",
    "        '1901-01-01',\n",
    "        '1909-09-09',\n",
    "        '2037-07-07',\n",
    "    ]\n",
    "    \n",
    "    date_matching_birthdate = '1902-02-02'\n",
    "    date_matching_birthyear = '1903-03-03'\n",
    "    \n",
    "    event_df = event_df[~event_df['date'].isna()]\n",
    "    event_df['date'] = event_df['date'].progress_map(lambda date: '-'.join(date.split('/')[::-1]))\n",
    "    event_df = event_df[~event_df['date'].isin(date_indicating_missing_information)]\n",
    "    \n",
    "    mask = event_df['date'] == date_matching_birthdate\n",
    "    event_df.loc[mask, 'date'] = exported_df.loc[event_df.loc[mask].index, '33-0.0']\n",
    "    \n",
    "    mask = event_df['date'] == date_matching_birthyear\n",
    "    event_df.loc[mask, 'date'] = exported_df.loc[event_df.loc[mask].index, '33-0.0'].apply(lambda date: date.split('-')[0])\n",
    "    \n",
    "    event_df = event_df.groupby('eid').agg({'event': list, 'date': list})\n",
    "    \n",
    "    def parse_and_sort_dates(row):\n",
    "        # TODO this will handle \"date_matching_birthyear\" cases incorrectly and put them before any birth events\n",
    "        dates = np.array(row['date'])\n",
    "        events = np.array(row['event'])\n",
    "    \n",
    "        sorted_indices = np.argsort(dates)\n",
    "        return {'event': events[sorted_indices], 'date': dates[sorted_indices]}\n",
    "    \n",
    "    event_df = event_df.progress_apply(parse_and_sort_dates, axis=1, result_type='expand')\n",
    "\n",
    "    return event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da4a32-9f41-4662-91d5-bd578498f55c",
   "metadata": {},
   "source": [
    "## Convert drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd7850-f2cb-4f08-8ea2-de7fe1c83b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_prescriptions = pd.read_csv('data/raw/drug_export/data/gp_scripts.txt', sep='\\t')\n",
    "presner = pd.read_json(\"data/raw/drug_export/out/result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a85d7-abf6-4ee0-8480-5d06e04a78c5",
   "metadata": {},
   "source": [
    "The code below is tailored to address a specific challenge in pharmacological data processing: the elimination of redundant compound entries from a dataset of drugs. This issue arises when drugs, which are often composed of various compounds, are decomposed into their constituent chemical entities. In this decomposition process, it's not uncommon to find that multiple compounds associated with a single drug actually share the same parent active compound. \n",
    "\n",
    "Such duplication can occur due to the presence of different salts, esters, or isomers of the same active molecule, which are chemically distinct but therapeutically equivalent. For instance, a drug might be listed with both its hydrochloride and sulfate salts, but both these forms have the same active parent compound.\n",
    "\n",
    "The code addresses this issue by first extracting the unique compound identifiers (ChEMBL IDs) from a comprehensive dataset of drugs. It then employs a batch processing approach to query an external database (ChEMBL), retrieving detailed information about these compounds, including their hierarchical relationships (i.e., which compounds are parent compounds and which are derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af655b83-0ed2-4cc8-becb-8936856bea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_chembl_data(chembl_ids_batch):\n",
    "    \"\"\"\n",
    "    Fetches molecule forms data from the ChEMBL database for a batch of ChEMBL IDs.\n",
    "\n",
    "    Args:\n",
    "    chembl_ids_batch (list): A list of ChEMBL IDs.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of molecule forms data for the given ChEMBL IDs.\n",
    "    \"\"\"\n",
    "    url = f'https://www.ebi.ac.uk/chembl/api/data/molecule_form/set/{\";\".join(chembl_ids_batch)}?format=json'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['molecule_forms']\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def get_unique_chembl_ids(chembl_ids):\n",
    "    \"\"\"\n",
    "    Filters and returns unique ChEMBL IDs, excluding those whose parent IDs are also in the list.\n",
    "\n",
    "    Args:\n",
    "    chembl_ids (list): A list of ChEMBL IDs.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of unique ChEMBL IDs after filtering.\n",
    "    \"\"\"\n",
    "    unique_ids = set(chembl_ids)\n",
    "    return [uid for uid in unique_ids if lut_parent_id.get(uid) not in unique_ids]\n",
    "\n",
    "# Process the data to extract unique ChEMBL IDs and group them\n",
    "unique_chembl_ids = presner['chemblid'].unique().tolist()\n",
    "grouped_lists = presner.groupby('fk')['chemblid'].apply(tuple).reset_index(name='chemblid_list')\n",
    "\n",
    "# Define the batch size for API requests\n",
    "batch_size = 50\n",
    "\n",
    "# Split the unique ChEMBL IDs into manageable batches\n",
    "chembl_id_batches = [unique_chembl_ids[i:i + batch_size] for i in range(0, len(unique_chembl_ids), batch_size)]\n",
    "\n",
    "# Initialize a Look-Up Table (LUT) for parent IDs\n",
    "lut_parent_id = {}\n",
    "\n",
    "# Fetch data for each batch and update the LUT\n",
    "for batch in tqdm(chembl_id_batches, desc=\"Fetching ChEMBL Data\"):\n",
    "    batch_data = fetch_chembl_data(batch)\n",
    "    for molecule in batch_data:\n",
    "        if not molecule['is_parent']:\n",
    "            lut_parent_id[molecule['molecule_chembl_id']] = molecule['parent_chembl_id']\n",
    "\n",
    "# Create a dictionary to determine which IDs to keep\n",
    "to_keep = {\n",
    "    fk: get_unique_chembl_ids(tuple(sorted(chemblid_list)))\n",
    "    for fk, chemblid_list in zip(grouped_lists['fk'], grouped_lists['chemblid_list'])\n",
    "}\n",
    "\n",
    "# Map the chemblid_list to each fk in the original df\n",
    "presner['to_keep'] = presner['fk'].map(to_keep)\n",
    "\n",
    "# Check if each chemblid is in the corresponding to_keep list\n",
    "presner['keep_row'] = presner.apply(lambda x: x['chemblid'] in x['to_keep'], axis=1)\n",
    "\n",
    "# Filter the dataframe to keep only the rows that are marked to keep\n",
    "presner = presner[presner['keep_row']].drop(columns=['to_keep', 'keep_row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e5d8a-189c-498b-ba3d-d26696d45002",
   "metadata": {},
   "outputs": [],
   "source": [
    "presner = presner.set_index(['TEXT_drug_name', 'TEXT_quantity'])\n",
    "ukb_prescriptions = ukb_prescriptions.set_index(['drug_name', 'quantity'])\n",
    "ukb_prescriptions.index.names = ['TEXT_drug_name', 'TEXT_quantity']\n",
    "\n",
    "ukb_prescriptions = ukb_prescriptions.join(presner, how='inner', lsuffix='_ukb', rsuffix='_presner')\n",
    "ukb_prescriptions = ukb_prescriptions.reset_index().set_index('eid')\n",
    "ukb_prescriptions = ukb_prescriptions[['chemblid', 'issue_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae05f71-2ea5-46f0-ab39-1e9d8ef79d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove drugs with low occurence as they could interfere with statistical learning methods\n",
    "mask = ukb_prescriptions['chemblid'].map(ukb_prescriptions['chemblid'].value_counts()) >= MIN_DRUG_OCCURENCE_THRESHOLD\n",
    "ukb_prescriptions = ukb_prescriptions[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b65bc1f-3c19-4948-8713-6ded68e2c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_prescriptions.columns = ['event', 'date']\n",
    "ukb_prescriptions = handle_event_df(ukb_prescriptions)\n",
    "ukb_prescriptions.columns = ['drug_codes', 'drug_dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57998a43-43e4-468d-b816-340561aa09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_df = exported_df.join(ukb_prescriptions, how='left')\n",
    "exported_df['drug_codes'] = exported_df['drug_codes'].progress_apply(lambda x: np.array([], dtype='<U13') if x is np.nan else x)\n",
    "exported_df['drug_dates'] = exported_df['drug_dates'].progress_apply(lambda x: np.array([], dtype='<U13') if x is np.nan else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67eec9b-86e5-4e7e-b256-766e5cbcd492",
   "metadata": {},
   "source": [
    "## Disease export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc8b78-16e1-4414-bea3-e3fb6bf52c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_onset_df = pd.read_csv('data/raw/disease_onset.csv', dtype=str).set_index('eid')\n",
    "disease_onset_df.index = disease_onset_df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801b108-1ff0-4cbc-8b00-48a907413585",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_onset_df = disease_onset_df.melt(var_name='event', value_name='date', ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01f584-81d1-4f45-905e-d0cb6667c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_onset_df = handle_event_df(disease_onset_df)\n",
    "disease_onset_df.columns = ['disease_codes', 'disease_dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fd0f5-cd43-4dbb-ba78-2d8037d66638",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_df = exported_df.join(disease_onset_df, how='left')\n",
    "exported_df['disease_codes'] = exported_df['disease_codes'].progress_apply(lambda x: np.array([], dtype='<U13') if x is np.nan else x)\n",
    "exported_df['disease_dates'] = exported_df['disease_dates'].progress_apply(lambda x: np.array([], dtype='<U13') if x is np.nan else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ada05-1623-494f-bae5-7063a79dcf66",
   "metadata": {},
   "source": [
    "# Save df as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f2d08-072e-4d76-9a6b-fa587e1288c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_df.to_parquet('data/processed/ukb.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e09248-056d-4f49-b204-2274a5d21696",
   "metadata": {},
   "source": [
    "# Save with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beea942-06f5-42ca-88f9-2665bc6b29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value, Sequence, ClassLabel, Dataset\n",
    "\n",
    "def get_dataset_dtype(dtype):\n",
    "    if pd.api.types.is_float_dtype(dtype):\n",
    "        return Value('float32')\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return Value('int32')\n",
    "    if pd.api.types.is_string_dtype(dtype):\n",
    "        return Value('string')\n",
    "    if pd.api.types.is_categorical_dtype(dtype):\n",
    "        return ClassLabel(names=dtype.categories.tolist())\n",
    "\n",
    "    raise TypeError\n",
    "\n",
    "features = {c:get_dataset_dtype(exported_df[c].dtype) for c in exported_df}\n",
    "features['eid'] = Value('int32')\n",
    "features['disease_codes'] =  Sequence(Value('string'))\n",
    "features['disease_dates'] =  Sequence(Value('string'))\n",
    "features['drug_codes'] =  Sequence(Value('string'))\n",
    "features['drug_dates'] =  Sequence(Value('string'))\n",
    "\n",
    "for c in exported_df:\n",
    "    if pd.api.types.is_categorical_dtype(exported_df[c].dtype):\n",
    "        exported_df[c] = exported_df[c].astype('object')\n",
    "\n",
    "dataset = Dataset.from_pandas(exported_df, features=Features(features))\n",
    "dataset.save_to_disk(\"data/processed/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858c33a-e2da-4cb6-abf9-196cbb3d93e4",
   "metadata": {},
   "source": [
    "# Additional helper files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c9e7c-8391-4e0b-877c-18ba8e7bec20",
   "metadata": {},
   "source": [
    "## Special values for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067ca58-86e2-4fba-869f-46bcbc3e519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed/helper_files/special_field_values.json', 'w') as f:\n",
    "    json.dump(special_values, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0b823-1d92-49b5-a8cf-9b57c6bc1bf9",
   "metadata": {},
   "source": [
    "## Field to modality dict for constructing different token sequenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4ecba-64bc-4a65-812c-71b6907a2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From personal:\n",
    "# remove 34: Year of birth and 52: Month of birth, and add 33: Date of birth\n",
    "# remove 21003: Age at recruitment, only needed for event dates, not as model input\n",
    "# remove 21000: Ethnic background, 6138: Qualicifactions, they are only used for filtering not as model input\n",
    "# replace 189 with 22189, the old downloaded UKB data contains this field under this id\n",
    "\n",
    "field_to_modality = {}\n",
    "\n",
    "for f in fields_by_file:\n",
    "    for field_id in fields_by_file[f]:\n",
    "        field_to_modality[field_id] = 'lab' if 'lab' in f else 'personal'\n",
    "\n",
    "del field_to_modality[21003]\n",
    "del field_to_modality[34]\n",
    "del field_to_modality[52]\n",
    "del field_to_modality[21000]\n",
    "del field_to_modality[6138]\n",
    "field_to_modality[33] = 'personal'\n",
    "field_to_modality[22189] = field_to_modality[189]\n",
    "del field_to_modality[189]\n",
    "\n",
    "with open('data/processed/helper_files/field_to_modality.json', 'w') as f:\n",
    "    json.dump(field_to_modality, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d89a4-5c78-4eac-ba6c-c012774b9401",
   "metadata": {},
   "source": [
    "## Field to time-invariancy for constructing date sequences during tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ad107-61d9-4809-bf4f-11ada66a9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_to_timevariant = {}\n",
    "\n",
    "for f in fields_by_file:\n",
    "    for field_id in fields_by_file[f]:\n",
    "        field_to_timevariant[field_id] = False if 'invariant' in f else True\n",
    "\n",
    "# Similarly to above\n",
    "del field_to_timevariant[21003]\n",
    "del field_to_timevariant[34]\n",
    "del field_to_timevariant[52]\n",
    "del field_to_timevariant[21000]\n",
    "del field_to_timevariant[6138]\n",
    "field_to_timevariant[33] = False\n",
    "field_to_timevariant[22189] = field_to_timevariant[189]\n",
    "del field_to_timevariant[189]\n",
    "\n",
    "\n",
    "with open('data/processed/helper_files/field_timevariant.json', 'w') as f:\n",
    "    json.dump(field_to_timevariant, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ac287-b971-4b6e-bf7a-ca11e111bee1",
   "metadata": {},
   "source": [
    "## Convert Hierarchical String Encoding 19 (ICD10) codes, into conversion dictionaries for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f943b3-bac6-4981-8471-2e45d7aaa55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehierstr_df = pd.read_csv('data/raw/ehierstring.txt', sep='\\t')\n",
    "icd_hier_df = ehierstr_df[ehierstr_df['encoding_id'] == 19]\n",
    "\n",
    "# Create a helper function to build the hierarchy recursively\n",
    "def build_hierarchy(df, node_id, depth):\n",
    "    node = df[df['code_id'] == node_id].iloc[0]\n",
    "\n",
    "    children = df[df['parent_id'] == node_id] if depth != 0 else pd.Series([], dtype=int)\n",
    "        \n",
    "    node_dict = {\n",
    "        'code_id': node['code_id'],\n",
    "        'value': node['value'],\n",
    "        'meaning': node['meaning'],\n",
    "        'children': [build_hierarchy(df, child['code_id'], depth - 1) for _, child in children.iterrows()] if not children.empty else []\n",
    "    }\n",
    "    return node_dict\n",
    "\n",
    "# Find root nodes (parent_id = 0)\n",
    "root_nodes = icd_hier_df[icd_hier_df['parent_id'] == 0]\n",
    "\n",
    "# Initialize a dictionary to store the hierarchical structure\n",
    "hierarchical_structure  = []\n",
    "\n",
    "# Create the final list of dictionaries\n",
    "for _, root_node in root_nodes.iterrows():\n",
    "    hierarchical_structure.append(build_hierarchy(icd_hier_df, root_node['code_id'], depth=2))\n",
    "\n",
    "\n",
    "# Initialize dictionaries\n",
    "code_to_chapter = {}\n",
    "code_to_block = {}\n",
    "block_to_chapter = {}\n",
    "chapter_to_name = {}\n",
    "block_to_name = {}\n",
    "code_to_name = {}\n",
    "\n",
    "# Iterate through the hierarchical_structure\n",
    "for chapter in hierarchical_structure:\n",
    "    chapter_value = chapter['value'][8:]  # remove \"Chapter \" from the front\n",
    "    chapter_meaning = chapter['meaning']\n",
    "\n",
    "    # Populate chapter_to_name dictionary\n",
    "    chapter_to_name[chapter_value] = chapter_meaning\n",
    "\n",
    "    # Iterate through blocks within the chapter\n",
    "    for block in chapter.get('children', []):\n",
    "        block_value = block['value'][6:] # remove \"Block \" from the front\n",
    "        block_meaning = block['meaning']\n",
    "\n",
    "        # Populate block_to_chapter dictionary\n",
    "        block_to_chapter[block_value] = chapter_value\n",
    "        block_to_name[block_value] = block_meaning\n",
    "\n",
    "        # Iterate through codes within the block\n",
    "        for code in block.get('children', []):\n",
    "            code_value = code['value']\n",
    "            code_meaning = code['meaning']\n",
    "\n",
    "            # Populate code_to_chapter and code_to_block dictionaries\n",
    "            code_to_chapter[code_value] = chapter_value\n",
    "            code_to_block[code_value] = block_value\n",
    "\n",
    "            # Populate code_to_name dictionary\n",
    "            code_to_name[code_value] = code_meaning\n",
    "\n",
    "with open('data/processed/helper_files/icd10_hierarchy.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'code_to_chapter': code_to_chapter,\n",
    "        'code_to_block': code_to_block,\n",
    "        'block_to_chapter': block_to_chapter,\n",
    "        'chapter_to_name': chapter_to_name,\n",
    "        'block_to_name': block_to_name,\n",
    "        'code_to_name': code_to_name,\n",
    "    }, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
